<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-129673183-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-129673183-1');
</script>

  <meta name=viewport content="width=800">
  <!-- <meta charset="utf-8" />  -->
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */

    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }

    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>
    <link rel="stylesheet" href="styles.css">
  <!-- <link rel="icon" type="image/png" href="images/icon.png"> -->
  <title>Nikhil Akalwadi</title>
  <link rel="icon" href="images/favicon_io/favicon-32x32.png"  type="image/x-icon">
  <!-- <link rel="icon" href= 
  "https://media.geeksforgeeks.org/wp-content/cdn-uploads/gfg_200X200.png" 
          type="image/x-icon">  -->
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <!-- Bio and Image -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="80%" valign="middle">
              <p align="center">
                <name style="font-size: 42;">Nikhil Akalwadi</name>
              </p>
		  
	      <p>&nbsp;</p>
              <p align="justify">
                I am Research Scholar at <strong><a href="https://cevi.co.in">KLETech Center of Excellence in Visual Intelligence (CEVI)</a> </strong>advised by <a href="https://kletech.irins.org/profile/159972">Prof. Uma Mudenagudi</a> and <a href="https://kletech.irins.org/profile/163625">Ramesh Ashok Tabib</a> where I work on development of computer vision algorithms in areas like Low-Level Vision Systems, Underwater Image Restoration and Enhancement.
              </p>

              <p align="justify">
                I completed my Bachelors in Electronics and Communication from KLE Technological University, Hubballi. I majorly worked on Low-Level Vision Systems and published in the area, advised by Prof Uma Mudenagudi, <a href="https://kletech.irins.org/profile/159977">Prof. Ujwala Patil</a> and Ramesh Ashok Tabib.
              </p>
                
              <p align=center margin=0px>
                <a href="mailto:akalwadinikhil@gmail.com"><img src="images/gmail.png" class="depth-effect" height="24"></a>&nbsp;
                <!-- <a href="data/ShivanandSheshappanavar_CV.pdf">CV</a> &nbsp/&nbsp -->
		<!-- <a href="data/Statement_of_Teaching_Philosophy.pdf">Teaching Philosophy</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=XKstxfMAAAAJ&hl=en"><img src="images/googleScholar.png" class="depth-effect" height="26"></a> &nbsp;
                <a href="https://orcid.org/0009-0003-6639-9288"><img src="images/orcid.png" class="depth-effect" height="26"></a>&nbsp;
                <a href="https://www.researchgate.net/profile/Nikhil-Akalwadi"><img src="images/researchgate.png" class="depth-effect" height="25"></a>&nbsp;
                <a href="https://github.com/lucciffer"><img src="images/github.png" class="depth-effect" height="24"></a> &nbsp;
                <a href="https://www.linkedin.com/in/akalwadinikhil/"><img src="images/linkedin.png" class="depth-effect" height="24"></a> &nbsp;
                <a href="https://twitter.com/lucciffer__"><img src="images/twitterX.png" class="depth-effect" height="26"></a>
                <a href="/data/Nikhil Akalwadi CV.pdf"><img src="images/cv.png" class="depth-effect" height="26"></a>

		<!-- <a href="https://www.facebook.com/groups/PhDinUS"> PhDinUS (Facebook) </a> -->
             </p>
	       <!-- I am a PhD Graduate from the Dept. of Computer and Information Sciences at the University of Delaware. I did my doctoral research at the VIMS Laboratory under the guidance of <a href="https://www.eecis.udel.edu/~chandra/"> Dr. Chandra Kambhamettu</a>. I had completed my Masters in Computer Science at Syracuse University, New York (2018). Previously, I worked as an IT Consultant at Oracle India Private Limited (2012-2016). I also hold a Master's and a Bachelor's degree in Computer Science and Engineering from RVCE (2012) and MSRIT (2009), Bengaluru, respectively. -->
	     

        <!-- <p align="justify" style="color:red;">
          <b><span color="red">
              Currently looking for a full Time PhD / Research Assistant / Research Intern        
            </span>
          </b>
        </p> -->
        

	    </td>
<!-- 	    <td width="15%">
              <img src='images/self.pnh' width=100%>
            </td> -->
            <td width="30%">
              <img src="images/nikhilakalwadi.png" width=140%>
            </td>
          </tr>
	  <tr>
            <td colspan="2">
              
	     <p>&nbsp;</p>

       
       <h2>Research Interests</h2>
       <p align="justify">
        My primary research area is Computer Vision (Underwater Image Restoration and Enhancement, Low-Level Vision, Multispectral Image Analysis).
             </p>
        <p align="justify"> 
        <h2>
          Recent Updates !!!
        </h2>
        <ul>
          <li><i><FONT COLOR="magenta">June 2024:</FONT></i> Our work on Image Denoising titled <strong>"HNN: Hierarchical Noise-Deinterlace Net Towards Image Denoising"</strong> is now <strong style="font-size: 16"><FONT COLOR="green">Published</FONT></strong> in the CVF Repository. <a href="https://openaccess.thecvf.com/content/CVPR2024W/PBVS/papers/Joshi_HNN_Hierarchical_Noise-Deinterlace_Net_Towards_Image_Denoising_CVPRW_2024_paper.pdf">(View publication here)</a></li> 


          <li><i><FONT COLOR="magenta">May 2024:</FONT> </i><strong>KLETech Center of Excellence in Visual Intelligence is Co-Organizing the <a href="https://3dvss.github.io">3D Vision Summer School @ IIIT Bangalore</a></strong> . I will be part of Volunteering Team for this event!!</li>

          <li><i><FONT COLOR="magenta">April 2024:</FONT></i> arXiv Pre-Print <strong style="font-size: 16"><FONT COLOR="green">Published</FONT></strong> titled <strong style="font-size: 16;">
            NTIRE 2024 challenge on low light image enhancement: Methods and results
          </strong> <strong>@ NTIRE Challenges, CVPR 2024</strong> (<a href="https://arxiv.org/abs/2404.14248">View Publication here</a>)</li>

          <li><i><FONT COLOR="magenta">April 2024:</FONT></i> arXiv Pre-Print <strong style="font-size: 16"><FONT COLOR="green">Published</FONT></strong> titled <strong style="font-size: 16;">
            The Ninth NTIRE 2024 Efficient Super-Resolution Challenge Report
          </strong> <strong>@ NTIRE Challenges, CVPR 2024</strong> (<a href="https://arxiv.org/abs/2404.10343">View Publication here</a>)</li>

          <li><i><FONT COLOR="magenta">April 2024:</FONT></i> arXiv Pre-Print <strong style="font-size: 16"><FONT COLOR="green">Published</FONT></strong> titled <strong style="font-size: 16;">
            NTIRE 2024 Challenge on Image Super-Resolution (4): Methods and Results
          </strong> <strong>@ NTIRE Challenges, CVPR 2024</strong> (<a href="https://arxiv.org/abs/2404.09790">View Publication here</a>)</li>

          <li> <i><FONT COLOR="magenta">April 2024:</FONT></i> Our work on Image denoising got <strong style="font-size: 16;"><FONT COLOR=green>Accepted</FONT></strong> in <strong>PBVS Workshop @ CVPR 2024</strong> with paper title <strong style="font-size: 16;">HNN: Hierarchical Noise-Deinterlace Net Towards Image Denoising</strong></a></b>.

          <li> <i><FONT COLOR="magenta">April 2024:</FONT></i> Our work on Low-Light Image Enhancement got <strong style="font-size: 16;"><FONT COLOR=green>Accepted</FONT></strong> in <strong>as Poster @ WiCV Workshop, CVPR 2024</strong> with paper title <strong style="font-size: 16;">ViD: Vision in Dark</strong></a></b>.

          <li> <i><FONT COLOR="magenta">December 2023:</FONT></i> Our work on Flare Detection and Removal in Images is now <strong style="font-size: 16;"><FONT COLOR=green>Published</FONT></strong> in <strong>PReMI 2023</strong> with paper title <strong style="font-size: 16;">DeFlare-Net: Flare Detection and Removal Network</strong></a></b>. (<a href="https://link.springer.com/chapter/10.1007/978-3-031-45170-6_48">View Publication here</a>)

          <li> <i><FONT COLOR="magenta">June 2023:</FONT></i> Our work on Flare Detection and Removal in Images got <strong style="font-size: 16;"><FONT COLOR=green>Accepted</FONT></strong> in <strong>PReMI 2023</strong> with paper title <strong style="font-size: 16;">DeFlare-Net: Flare Detection and Removal Network</strong></a></b>.

          <li> <i><FONT COLOR="magenta">October 2023:</FONT></i> Our work on Low-Light Image Enhancement got <strong style="font-size: 16;"><FONT COLOR=green>Accepted</FONT></strong> in <strong>WiCV Workshop @ ICCV 2023</strong> with paper title <strong>LightNet: Generative Model for Enhancement of Low-Light Images</strong></a></b>.

          <li> <i><FONT COLOR="magenta">March 2023:</FONT></i> We achieved <strong style="font-size: 16;"><FONT COLOR=green>8th rank globally</FONT></strong> in <strong>MIPI Challenge @ CVPR 2023</strong>.

          <!-- <li> <i><FONT COLOR="magenta">July 2023:</FONT></i> We <strong style="font-size: 16;"><FONT COLOR=green>published a joint report</FONT></strong> on flare detection and removal in <strong>MIPI Challenge @ CVPR 2023</strong> with report title <strong>MIPI 2023 Challenge on Nighttime Flare Removal: Methods and Results</strong></a></b>. -->
          <!-- <li> <i>April 2022:</i> We achieved <strong style="font-size: 16;"><FONT COLOR=green>13th rank globally</FONT></strong> in <strong>NTIRE Challenge @ CVPR 2022</strong>. -->
          <!-- <li> <i>July 2022:</i> We <strong style="font-size: 16;"><FONT COLOR=green>published a joint report</FONT></strong> on night photography rendering in <strong>NTIRE Challenge @ CVPR 2022</strong> with report title <strong>NTIRE 2022 Challenge on Night Photography Rendering</strong></a></b>. -->
              
          </ul>
      </p>
        <h2>Selected Works</h2>
	   </td>
	  </tr>
      
        </table>
	<!-- <div class = "row">
        <table width="100%" align="center" margin-top=100px>
            <tr>
              <td align="center" width="16%" style = "vertical-align: middle">
                <img src = "/images/UD.png" width="60%">
              </td>
		    
	      <td align="center" width="16%" style = "vertical-align: middle">
                <img src = "/images/syracuse.png" width="30%">
              </td>

              <td align="center" width="14%" style = "vertical-align: middle">
                <img src = "/images/oracle.png" width="50%">
              </td>

              <td align="center" width="14%" style = "vertical-align: middle">
                <img src = "/images/infineon.png" width="60%">
              </td>
            </tr>

            <tr>
                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">PhD, CS<br>University of Delaware<br>2018 - 2023</td>
		    
		<td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">MS, CS<br>Syracuse University<br>2016 - 2018</td>

                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">IT Consultant<br>Oracle<br>2012 - 2016</td>

                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">Research Intern<br>Infineon<br>2011 - 2012</td> -->

                <!-- <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">MTech<br>R.V. College of Engineering<br>2010 - 2012</td>

                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">BE<br>M.S.Ramaiah Institute of Technology<br>2005 - 2009</td> -->
          <!-- </tr>

          </table>
      </div> -->

        <!-- Research Interest -->
        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td width="100%" valign="middle">
        <heading>Research</heading>
        <p align="justify">
          My research interests are to develop deep learning algorithms for 3D computer vision problems and create end-to-end solution pipelines. My long-term goal is to build a mobile-based assistant for the visually impaired to help them navigate the real world.
        </p>
      </td>
    </tr>
        </table> -->


        <!-- #######Publications -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		
          <!-- HNN -->
          <tr>
            <td width="35%">
              <img src='images/hnn.gif' class="depth-effect" width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://openaccess.thecvf.com/content/CVPR2024W/PBVS/html/Joshi_HNN_Hierarchical_Noise-Deinterlace_Net_Towards_Image_Denoising_CVPRW_2024_paper.html">
                  <papertitle style="font-size: 18">HNN: Hierarchical Noise-Deinterlace Net Towards Image Denoising</papertitle>
                </a>
                <br>
                Amogh Joshi, <strong style="font-size: 16">Nikhil Akalwadi</strong>, Chinmayee Mandi, Chaitra Desai, Ramesh Ashok Tabib, Ujwala Patil, Uma Mudenagudi
                <br>
                <em>The 20th Workshop on Perception Beyond Visible Spectrum (PBVS), <strong>CVPRW 2024 </strong><strong style="font-size: 16;"></strong>. </em>
                <br> 
                <a href="https://sites.google.com/view/hnndenoising/home">Project Page</a> /
                <a href="/data/HNN-PBVS-CPVRW2024-Poster.pdf">Poster PDF</a> / 
                <a href="data/Joshi_HNN.bib">BibTex</a>
                <!-- <p align="justify">We present a framework inspired by human cognition to decompose point clouds into four primitive 3D shapes (plane, cylinder, cone, and sphere) and enable machines to understand the objects irrespective of its orientations. We demonstrate the results of our proposed methodology for SO(3) invariant decomposition on TraceParts Dataset, and show the generalizability of proposed IPD-Net as plugin for downstream task on classification of point clouds.
                  </p> -->
            </td>
          </tr>

          <!-- ViD Poster -->
          <tr>
            <td width="35%">
              <img src='images/vid-thumbnail.png' class="depth-effect" width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <!-- <a href="https://openaccess.thecvf.com/content/CVPR2024W/PBVS/html/Joshi_HNN_Hierarchical_Noise-Deinterlace_Net_Towards_Image_Denoising_CVPRW_2024_paper.html"> -->
                  <papertitle style="font-size: 18">ViD: Vision in Dark</papertitle>
                </a>
                <br>
                Sampada Malagi, Amogh Joshi, <strong style="font-size: 16">Nikhil Akalwadi</strong>, Chaitra Desai, Ramesh Ashok Tabib, Ujwala Patil, Uma Mudenagudi
                <br>
                <em>Women in Computer Vision (WiCV), <strong>CVPRW 2024 </strong><strong style="font-size: 16;"></strong>. </em>
                <br> 
                <a href="data/poster_67.pdf">Poster PDF</a> 
                <!-- <a href="data/Joshi_HNN.bib">BibTex</a> -->
                <!-- <p align="justify">We present a framework inspired by human cognition to decompose point clouds into four primitive 3D shapes (plane, cylinder, cone, and sphere) and enable machines to understand the objects irrespective of its orientations. We demonstrate the results of our proposed methodology for SO(3) invariant decomposition on TraceParts Dataset, and show the generalizability of proposed IPD-Net as plugin for downstream task on classification of point clouds.
                  </p> -->
            </td>
          </tr>




          <!-- LightNet -->
          <tr>
            <td width="35%">
              <img src='images/lightnet.gif' class="depth-effect" width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://openaccess.thecvf.com/content/ICCV2023W/WiCV/html/Desai_LightNet_Generative_Model_for_Enhancement_of_Low-Light_Images_ICCVW_2023_paper.html">
                  <papertitle style="font-size: 18">LightNet: Generative Model for Enhancement of Low-Light Images</papertitle>
                </a>
                <br>
                Chaitra Desai, <strong style="font-size: 16">Nikhil Akalwadi</strong>, Amogh Joshi, Sampada Malagi, Chinmayee Mandi, Ramesh Ashok Tabib, Ujwala Patil, Uma Mudenagudi
                <br>
                <em>Women in Computer Vision (WiCV), <strong>ICCVW 2023 </strong><strong style="font-size: 16;"><FONT COLOR=green>(Oral)</FONT></strong>. </em>
                <br> 
                <a href="https://sites.google.com/view/lightnet-llie/home">Project Page</a> /
                <a href="/data/[WiCV ICCV 2023] LightNet Poster.pdf">Poster PDF</a> / 
                <a href="data/cd2023LightNet.bib">BibTex</a>
                <!-- <p align="justify">We present a framework inspired by human cognition to decompose point clouds into four primitive 3D shapes (plane, cylinder, cone, and sphere) and enable machines to understand the objects irrespective of its orientations. We demonstrate the results of our proposed methodology for SO(3) invariant decomposition on TraceParts Dataset, and show the generalizability of proposed IPD-Net as plugin for downstream task on classification of point clouds.
                  </p> -->
            </td>
          </tr>


          <!-- MIPI 2023 -->
          <tr>
            <td width="35%">
              <img src='images/MIPI2023.png' class="depth-effect" width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://openaccess.thecvf.com/content/CVPR2023W/MIPI/html/Dai_MIPI_2023_Challenge_on_Nighttime_Flare_Removal_Methods_and_Results_CVPRW_2023_paper.html">
                  <papertitle style="font-size: 18"> MIPI 2023 Challenge on Nighttime Flare Removal: Methods and Results</papertitle>
                </a>
                <br>
                
                ..., <strong style="font-size: 16">Nikhil Akalwadi</strong>, Ankit Raichur, Vinod Patil, Allabakash G, Swaroop A, Amogh Joshi, Chaitra Desai, Ramesh Ashok Tabib, Ujwala Patil, Uma Mudenagudi, ...
                <br>
                <em>Mobile Intelligent Photography & Imaging (MIPI), <strong>CVPR 2023</strong>. </em>
                <br> 
                <a href="https://sites.google.com/view/mipi-2023-challenge/home">Project Page</a> /
                <a href="http://arxiv.org/abs/2305.13770">arXiv</a> / 
                <a href="data/Dai2023MIPI.bib">BibTex</a>
                <!-- <p align="justify">We present a framework inspired by human cognition to decompose point clouds into four primitive 3D shapes (plane, cylinder, cone, and sphere) and enable machines to understand the objects irrespective of its orientations. We demonstrate the results of our proposed methodology for SO(3) invariant decomposition on TraceParts Dataset, and show the generalizability of proposed IPD-Net as plugin for downstream task on classification of point clouds.
                  </p> -->
            </td>
          </tr>

          <!-- Deflare-Net PReMi -->
          <tr>
            <td width="35%">
              <img src='images/deflarenet.gif' class="depth-effect" width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://link.springer.com/chapter/10.1007/978-3-031-45170-6_48">
                  <papertitle style="font-size: 18">DeFlare-Net: Flare Detection and Removal Network</papertitle>
                </a>
                <br>
                Allabakash Ghodesawar, Vinod Patil, Ankit Raichur, Swaroop Adrashyappanamath, Sampada Malagi, <strong style="font-size: 16">Nikhil Akalwadi</strong>, Chaitra Desai, Ramesh Ashok Tabib, Ujwala Patil & Uma Mudenagudi 
                <br>
                <em>Pattern Recognition and Marchine Intelligence <strong>PReMI 2023</strong><strong style="font-size: 16;"><FONT COLOR=green> (Oral)</FONT></strong></em>
                <br> 
                <a href="https://sites.google.com/view/deflare-net/home">Project Page</a> /
                <a href="https://www.youtube.com/watch?v=NcAzd9BHA-8&t=26s">Video</a> / 
                <a href="data/dfn.bib">BibTex</a>
                <!-- <p align="justify">
                  We propose a novel Metric-K Nearest Neighbor (M-KNN) to facilitate topology aware learning in point clouds. Recent work rely on Ball queries or K-Nearest-Neighbor (KNN) for local feature extraction of point clouds and finds challenges in retaining topological information. M-KNN employes a generalised Minkowski distance in the KNN search algorithm for topological representation of point clouds. M-KNN enables state-of-the-art point cloud methods to perform topology aware downstream tasks.

                  </p> -->
            </td>
          </tr>



          <!-- NTIRE 2022 challenge -->
          <tr>
            <td width="35%">
              <img src='images/NITRE2022.png' class="depth-effect" width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Ershov_NTIRE_2022_Challenge_on_Night_Photography_Rendering_CVPRW_2022_paper.html">
                  <papertitle style="font-size: 18">NTIRE 2022 Challenge on Night Photography Rendering</papertitle>
                </a>
                <br>
                ..., Chaitra Desai, <strong style="font-size: 16">Nikhil Akalwadi</strong>, Amogh Joshi, Chinmayee Mandi, Sampada Malagi, Akash Uppin, Sai Sudheer Reddy, Ramesh Ashok Tabib, Ujwala Patil, Uma Mudenagudi
                <br>
                <em>New Trends in Image Restoration and Enhancement (NTIRE), <strong>CVPR 2022</strong></em>
                <br> 
                <a href="https://sites.google.com/view/ntire2022challenge/home">Project Page</a> /
                <a href="data/ntire22.bib">BibTex</a>
                <!-- <p align="justify">
                  We propose a novel Metric-K Nearest Neighbor (M-KNN) to facilitate topology aware learning in point clouds. Recent work rely on Ball queries or K-Nearest-Neighbor (KNN) for local feature extraction of point clouds and finds challenges in retaining topological information. M-KNN employes a generalised Minkowski distance in the KNN search algorithm for topological representation of point clouds. M-KNN enables state-of-the-art point cloud methods to perform topology aware downstream tasks.

                  </p> -->
            </td>
          </tr>




        <!-- Teaching -->
        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td width="100%" valign="middle">
                <heading>Teaching</heading>
								<p>
									I have been the Instructor for the following course:
										<ul>
											<li>CISC210: Introduction to Systems Programming [Summer 2020]</li>
										</ul>
                  I have been the Lead Teaching Assistant for the following course:
                    <ul>
                      <li> CISC210: Introduction to Systems Programming at the University of Delaware[Fall 2022, Spring 2022, Spring 2021, Fall 2020, Spring 2020, Fall 2019, Spring 2019] </li>
                    </ul>
									I have been the Teaching Assistant for the following courses:
	                  <ul>
	                    <li> CISC220: Data Structures at the University of Delaware [Fall 2021] </li>
											<li> CISC101: Principles of Computing at the University of Delaware [Winter 2021] </li>
	                    <li> CISC662: Advanced Computer Architecture at the University of Delaware [Fall 2018]</li>
	                  </ul>
                </p>
	   Some <a href="https:/related.html">related papers</a> to mine.
              </td>
            </tr>
        </table> -->

        <!-- Reference -->
        
      <!-- </td>
    </tr> -->
  </table>
  <p align="right"><a href="https://jonbarron.info/">[Website Credits]</a></p>
</body>

</html>
